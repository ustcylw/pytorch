{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e483dfae",
   "metadata": {},
   "source": [
    "# define layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7cd00b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T03:23:38.875381Z",
     "start_time": "2021-08-06T03:23:38.873309Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a5fb8bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T03:23:39.294526Z",
     "start_time": "2021-08-06T03:23:39.283797Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDense,self).__init__()\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(4,4)) for i in range(4)])\n",
    "        self.params.append(nn.Parameter(torch.randn(4,1)))\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i in range(len(self.params)):\n",
    "            x = torch.mm(x,self.params[i])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f13d9632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T03:23:39.694647Z",
     "start_time": "2021-08-06T03:23:39.682660Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDictDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDictDense, self).__init__()\n",
    "        self.params = nn.ParameterDict({\n",
    "                'linear1': nn.Parameter(torch.randn(4, 4)),\n",
    "                'linear2': nn.Parameter(torch.randn(4, 1))\n",
    "        })\n",
    "        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # 新增\n",
    "\n",
    "    def forward(self, x, choice='linear1'):\n",
    "        return torch.mm(x, self.params[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1694856b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T03:23:40.204501Z",
     "start_time": "2021-08-06T03:23:40.186791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['linear1', 'linear2', 'linear3']) odict_items([('linear1', Parameter containing:\n",
      "tensor([[-1.2834,  0.5038,  0.4842,  1.1676],\n",
      "        [ 0.9334, -0.4258, -1.0969,  0.0597],\n",
      "        [ 0.4293, -0.0191, -1.2807,  0.6863],\n",
      "        [ 1.5512,  0.9490, -0.8362, -2.3339]], requires_grad=True)), ('linear2', Parameter containing:\n",
      "tensor([[-0.4712],\n",
      "        [ 0.8981],\n",
      "        [-0.1326],\n",
      "        [ 0.2472]], requires_grad=True)), ('linear3', Parameter containing:\n",
      "tensor([[ 0.0543, -1.3508],\n",
      "        [-1.2271, -0.2308],\n",
      "        [ 0.4370,  0.8450],\n",
      "        [ 1.1112,  1.0325]], requires_grad=True))])\n",
      "x: torch.Size([1, 4])\n",
      "y: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "net1 = MyDictDense()\n",
    "# print(net1)\n",
    "\n",
    "print(net1.params.keys(),net1.params.items())\n",
    "\n",
    "x = torch.ones(1, 4)\n",
    "# y = net1(x, 'linear1')\n",
    "# y = net1(x, 'linear2')\n",
    "y = net1(x, 'linear3')\n",
    "# y = net1(x)\n",
    "print(f'x: {x.shape}')\n",
    "print(f'y: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa0bf77c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T03:24:59.073867Z",
     "start_time": "2021-08-06T03:24:59.054999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): MyDictDense(\n",
      "    (params): ParameterDict(\n",
      "        (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "        (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "    )\n",
      "  )\n",
      "  (1): MyDense(\n",
      "    (params): ParameterList(\n",
      "        (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (3): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (4): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tensor([[30.9840]], grad_fn=<MmBackward>)\n",
      "x: torch.Size([1, 4])\n",
      "y: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "layer1 = MyDense()\n",
    "layer2 = MyDictDense()\n",
    "\n",
    "net = nn.Sequential(layer2,layer1)\n",
    "print(net)\n",
    "print(net(x))\n",
    "\n",
    "x = torch.ones(1, 4)\n",
    "y = net(x)\n",
    "print(f'x: {x.shape}')\n",
    "print(f'y: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab493ab",
   "metadata": {},
   "source": [
    "refs:\n",
    "\n",
    "    https://www.cnblogs.com/sdu20112013/p/12144843.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae754f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee9ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a454d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1783e824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T04:32:09.050788Z",
     "start_time": "2021-08-06T04:32:09.033515Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inherit from Function\n",
    "class LinearFunction(torch.autograd.Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        # ctx在这里类似self，ctx的属性可以在backward中调用\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "\n",
    "# #调用自定义的自动求导函数\n",
    "# linear = LinearFunction.apply(*args) #前向传播\n",
    "# linear.backward()#反向传播\n",
    "# linear.grad_fn.apply(*args)#反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5fd2bc4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T04:32:13.902976Z",
     "start_time": "2021-08-06T04:32:13.899004Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_features, output_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # nn.Parameter is a special kind of Variable, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # 这个很重要！ Parameters是默认需要梯度的！\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters can never be volatile and, different than Variables,\n",
    "        # they require gradients by default.\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        self.weight.data.uniform_(-0.1, 0.1)\n",
    "        if bias is not None:\n",
    "            self.bias.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # See the autograd section for explanation of what happens here.\n",
    "        return LinearFunction.apply(input, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0717c527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c2f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71529da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32c62d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8718144f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T04:26:56.319589Z",
     "start_time": "2021-08-06T04:26:56.316812Z"
    }
   },
   "outputs": [],
   "source": [
    "class MulConstant(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor, constant):\n",
    "        # ctx is a context object that can be used to stash information\n",
    "        # for backward computation\n",
    "        ctx.constant = constant\n",
    "        return tensor * constant\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        return grad_output * ctx.constant, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f032a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68981ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840479f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35a76e73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T04:24:32.335512Z",
     "start_time": "2021-08-06T04:24:32.333520Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7b9f788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T04:25:18.066126Z",
     "start_time": "2021-08-06T04:25:18.055520Z"
    }
   },
   "outputs": [],
   "source": [
    "class Exp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i.exp()\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        result, = ctx.saved_tensors\n",
    "        return grad_output * result\n",
    "\n",
    "#Use it by calling the apply method:\n",
    "y = Exp.apply(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d5d931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8b5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c474b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a25049f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T04:25:11.483137Z",
     "start_time": "2021-08-06T04:25:11.093536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 33542972.0\n",
      "1 loss: 31547180.0\n",
      "2 loss: 33672980.0\n",
      "3 loss: 34038432.0\n",
      "4 loss: 29089642.0\n",
      "5 loss: 19962214.0\n",
      "6 loss: 11322884.0\n",
      "7 loss: 5815654.5\n",
      "8 loss: 3044255.5\n",
      "9 loss: 1767338.0\n",
      "10 loss: 1165894.625\n",
      "11 loss: 854570.5625\n",
      "12 loss: 670973.9375\n",
      "13 loss: 548338.5\n",
      "14 loss: 458422.625\n",
      "15 loss: 388474.0\n",
      "16 loss: 332230.46875\n",
      "17 loss: 285971.09375\n",
      "18 loss: 247433.40625\n",
      "19 loss: 215083.984375\n",
      "20 loss: 187737.6875\n",
      "21 loss: 164504.6875\n",
      "22 loss: 144625.28125\n",
      "23 loss: 127528.296875\n",
      "24 loss: 112762.0703125\n",
      "25 loss: 99960.0859375\n",
      "26 loss: 88839.421875\n",
      "27 loss: 79138.3984375\n",
      "28 loss: 70649.90625\n",
      "29 loss: 63204.328125\n",
      "30 loss: 56657.921875\n",
      "31 loss: 50881.29296875\n",
      "32 loss: 45774.421875\n",
      "33 loss: 41249.96484375\n",
      "34 loss: 37226.05078125\n",
      "35 loss: 33642.36328125\n",
      "36 loss: 30448.8203125\n",
      "37 loss: 27595.29296875\n",
      "38 loss: 25041.794921875\n",
      "39 loss: 22752.044921875\n",
      "40 loss: 20697.447265625\n",
      "41 loss: 18850.796875\n",
      "42 loss: 17187.794921875\n",
      "43 loss: 15688.626953125\n",
      "44 loss: 14334.4189453125\n",
      "45 loss: 13109.5009765625\n",
      "46 loss: 12000.77734375\n",
      "47 loss: 10995.9716796875\n",
      "48 loss: 10083.896484375\n",
      "49 loss: 9254.853515625\n",
      "50 loss: 8500.2841796875\n",
      "51 loss: 7814.5283203125\n",
      "52 loss: 7190.615234375\n",
      "53 loss: 6621.375\n",
      "54 loss: 6101.6748046875\n",
      "55 loss: 5626.53466796875\n",
      "56 loss: 5191.5166015625\n",
      "57 loss: 4793.49072265625\n",
      "58 loss: 4428.3466796875\n",
      "59 loss: 4093.49658203125\n",
      "60 loss: 3786.5712890625\n",
      "61 loss: 3504.656005859375\n",
      "62 loss: 3245.55419921875\n",
      "63 loss: 3007.085205078125\n",
      "64 loss: 2787.51953125\n",
      "65 loss: 2585.134033203125\n",
      "66 loss: 2398.646240234375\n",
      "67 loss: 2226.667236328125\n",
      "68 loss: 2068.00830078125\n",
      "69 loss: 1921.441162109375\n",
      "70 loss: 1785.9779052734375\n",
      "71 loss: 1660.9642333984375\n",
      "72 loss: 1545.3365478515625\n",
      "73 loss: 1438.2513427734375\n",
      "74 loss: 1339.1051025390625\n",
      "75 loss: 1247.2518310546875\n",
      "76 loss: 1162.1324462890625\n",
      "77 loss: 1083.1878662109375\n",
      "78 loss: 1009.994384765625\n",
      "79 loss: 942.0734252929688\n",
      "80 loss: 879.0672607421875\n",
      "81 loss: 820.4876098632812\n",
      "82 loss: 766.0673828125\n",
      "83 loss: 715.4686279296875\n",
      "84 loss: 668.4002685546875\n",
      "85 loss: 624.623779296875\n",
      "86 loss: 583.8648071289062\n",
      "87 loss: 545.9298095703125\n",
      "88 loss: 510.60382080078125\n",
      "89 loss: 477.70123291015625\n",
      "90 loss: 447.0292053222656\n",
      "91 loss: 418.422607421875\n",
      "92 loss: 391.7572021484375\n",
      "93 loss: 366.8828125\n",
      "94 loss: 343.6807556152344\n",
      "95 loss: 322.01031494140625\n",
      "96 loss: 301.796142578125\n",
      "97 loss: 282.9562072753906\n",
      "98 loss: 265.3695983886719\n",
      "99 loss: 248.93280029296875\n",
      "100 loss: 233.5734405517578\n",
      "101 loss: 219.19577026367188\n",
      "102 loss: 205.75314331054688\n",
      "103 loss: 193.17001342773438\n",
      "104 loss: 181.39010620117188\n",
      "105 loss: 170.36512756347656\n",
      "106 loss: 160.0548095703125\n",
      "107 loss: 150.3984375\n",
      "108 loss: 141.3505859375\n",
      "109 loss: 132.88613891601562\n",
      "110 loss: 124.96005249023438\n",
      "111 loss: 117.52980041503906\n",
      "112 loss: 110.56655883789062\n",
      "113 loss: 104.02544403076172\n",
      "114 loss: 97.89031982421875\n",
      "115 loss: 92.13529968261719\n",
      "116 loss: 86.73469543457031\n",
      "117 loss: 81.66522979736328\n",
      "118 loss: 76.90076446533203\n",
      "119 loss: 72.42562103271484\n",
      "120 loss: 68.22152709960938\n",
      "121 loss: 64.27043151855469\n",
      "122 loss: 60.56128692626953\n",
      "123 loss: 57.07245635986328\n",
      "124 loss: 53.79411315917969\n",
      "125 loss: 50.70951843261719\n",
      "126 loss: 47.80801010131836\n",
      "127 loss: 45.08072280883789\n",
      "128 loss: 42.51239776611328\n",
      "129 loss: 40.09756851196289\n",
      "130 loss: 37.823917388916016\n",
      "131 loss: 35.68442916870117\n",
      "132 loss: 33.66960144042969\n",
      "133 loss: 31.77005386352539\n",
      "134 loss: 29.983678817749023\n",
      "135 loss: 28.30160903930664\n",
      "136 loss: 26.716129302978516\n",
      "137 loss: 25.223302841186523\n",
      "138 loss: 23.81664276123047\n",
      "139 loss: 22.491104125976562\n",
      "140 loss: 21.2410888671875\n",
      "141 loss: 20.06228256225586\n",
      "142 loss: 18.95193862915039\n",
      "143 loss: 17.903423309326172\n",
      "144 loss: 16.914392471313477\n",
      "145 loss: 15.982403755187988\n",
      "146 loss: 15.103679656982422\n",
      "147 loss: 14.273697853088379\n",
      "148 loss: 13.491464614868164\n",
      "149 loss: 12.75289249420166\n",
      "150 loss: 12.055516242980957\n",
      "151 loss: 11.3980131149292\n",
      "152 loss: 10.777495384216309\n",
      "153 loss: 10.190949440002441\n",
      "154 loss: 9.63758659362793\n",
      "155 loss: 9.11453914642334\n",
      "156 loss: 8.620881080627441\n",
      "157 loss: 8.154642105102539\n",
      "158 loss: 7.71398401260376\n",
      "159 loss: 7.298030376434326\n",
      "160 loss: 6.904879570007324\n",
      "161 loss: 6.533371448516846\n",
      "162 loss: 6.182103633880615\n",
      "163 loss: 5.8508477210998535\n",
      "164 loss: 5.5368547439575195\n",
      "165 loss: 5.240850448608398\n",
      "166 loss: 4.960688591003418\n",
      "167 loss: 4.695690631866455\n",
      "168 loss: 4.4452667236328125\n",
      "169 loss: 4.208472728729248\n",
      "170 loss: 3.984853506088257\n",
      "171 loss: 3.773026704788208\n",
      "172 loss: 3.5725529193878174\n",
      "173 loss: 3.3830974102020264\n",
      "174 loss: 3.203974485397339\n",
      "175 loss: 3.034308671951294\n",
      "176 loss: 2.8740410804748535\n",
      "177 loss: 2.7221455574035645\n",
      "178 loss: 2.578599452972412\n",
      "179 loss: 2.4426050186157227\n",
      "180 loss: 2.3142151832580566\n",
      "181 loss: 2.1925852298736572\n",
      "182 loss: 2.0772554874420166\n",
      "183 loss: 1.9682236909866333\n",
      "184 loss: 1.864987850189209\n",
      "185 loss: 1.7674819231033325\n",
      "186 loss: 1.674944519996643\n",
      "187 loss: 1.5872637033462524\n",
      "188 loss: 1.5043377876281738\n",
      "189 loss: 1.4258631467819214\n",
      "190 loss: 1.3513818979263306\n",
      "191 loss: 1.2808763980865479\n",
      "192 loss: 1.2142473459243774\n",
      "193 loss: 1.1510920524597168\n",
      "194 loss: 1.091303825378418\n",
      "195 loss: 1.0346001386642456\n",
      "196 loss: 0.9809477925300598\n",
      "197 loss: 0.930063784122467\n",
      "198 loss: 0.8817981481552124\n",
      "199 loss: 0.8361148238182068\n",
      "200 loss: 0.7928597331047058\n",
      "201 loss: 0.7519065737724304\n",
      "202 loss: 0.7131120562553406\n",
      "203 loss: 0.6762816309928894\n",
      "204 loss: 0.6413990259170532\n",
      "205 loss: 0.6082778573036194\n",
      "206 loss: 0.577002227306366\n",
      "207 loss: 0.5472844839096069\n",
      "208 loss: 0.5191535949707031\n",
      "209 loss: 0.49239787459373474\n",
      "210 loss: 0.4670671224594116\n",
      "211 loss: 0.4430791139602661\n",
      "212 loss: 0.4202878475189209\n",
      "213 loss: 0.3987758159637451\n",
      "214 loss: 0.37835797667503357\n",
      "215 loss: 0.3589288890361786\n",
      "216 loss: 0.3406325578689575\n",
      "217 loss: 0.323177695274353\n",
      "218 loss: 0.30659666657447815\n",
      "219 loss: 0.2909708619117737\n",
      "220 loss: 0.27613019943237305\n",
      "221 loss: 0.2620091140270233\n",
      "222 loss: 0.2486523687839508\n",
      "223 loss: 0.2359755039215088\n",
      "224 loss: 0.22399136424064636\n",
      "225 loss: 0.21257486939430237\n",
      "226 loss: 0.20174504816532135\n",
      "227 loss: 0.19153772294521332\n",
      "228 loss: 0.1817319393157959\n",
      "229 loss: 0.17250308394432068\n",
      "230 loss: 0.1636980175971985\n",
      "231 loss: 0.1554177850484848\n",
      "232 loss: 0.14754822850227356\n",
      "233 loss: 0.14005595445632935\n",
      "234 loss: 0.1329469233751297\n",
      "235 loss: 0.12619708478450775\n",
      "236 loss: 0.1198306530714035\n",
      "237 loss: 0.11376824975013733\n",
      "238 loss: 0.1080513671040535\n",
      "239 loss: 0.10256975144147873\n",
      "240 loss: 0.09736154228448868\n",
      "241 loss: 0.09245708584785461\n",
      "242 loss: 0.08777271956205368\n",
      "243 loss: 0.08334850519895554\n",
      "244 loss: 0.07914720475673676\n",
      "245 loss: 0.07515951246023178\n",
      "246 loss: 0.07136622816324234\n",
      "247 loss: 0.06778540462255478\n",
      "248 loss: 0.06436585634946823\n",
      "249 loss: 0.061131805181503296\n",
      "250 loss: 0.05803676322102547\n",
      "251 loss: 0.05515438690781593\n",
      "252 loss: 0.05237599462270737\n",
      "253 loss: 0.049743909388780594\n",
      "254 loss: 0.04724153131246567\n",
      "255 loss: 0.044872086495161057\n",
      "256 loss: 0.042613644152879715\n",
      "257 loss: 0.04048774391412735\n",
      "258 loss: 0.038447678089141846\n",
      "259 loss: 0.03654465079307556\n",
      "260 loss: 0.03472062572836876\n",
      "261 loss: 0.0329715870320797\n",
      "262 loss: 0.03132195025682449\n",
      "263 loss: 0.029772493988275528\n",
      "264 loss: 0.028279362246394157\n",
      "265 loss: 0.026870040223002434\n",
      "266 loss: 0.025519033893942833\n",
      "267 loss: 0.024242613464593887\n",
      "268 loss: 0.023036940023303032\n",
      "269 loss: 0.021900411695241928\n",
      "270 loss: 0.020816795527935028\n",
      "271 loss: 0.019781580194830894\n",
      "272 loss: 0.018812697380781174\n",
      "273 loss: 0.017882440239191055\n",
      "274 loss: 0.016988810151815414\n",
      "275 loss: 0.01615227200090885\n",
      "276 loss: 0.015372354537248611\n",
      "277 loss: 0.014608824625611305\n",
      "278 loss: 0.013880453072488308\n",
      "279 loss: 0.013203130103647709\n",
      "280 loss: 0.01256103441119194\n",
      "281 loss: 0.011943190358579159\n",
      "282 loss: 0.011357322335243225\n",
      "283 loss: 0.010804010555148125\n",
      "284 loss: 0.01028162706643343\n",
      "285 loss: 0.00977001991122961\n",
      "286 loss: 0.00930069014430046\n",
      "287 loss: 0.008851392194628716\n",
      "288 loss: 0.00842311605811119\n",
      "289 loss: 0.008012871257960796\n",
      "290 loss: 0.0076287961564958096\n",
      "291 loss: 0.0072664618492126465\n",
      "292 loss: 0.006922850850969553\n",
      "293 loss: 0.006585337221622467\n",
      "294 loss: 0.006272965110838413\n",
      "295 loss: 0.005977021064609289\n",
      "296 loss: 0.00569914560765028\n",
      "297 loss: 0.00542998593300581\n",
      "298 loss: 0.00517447292804718\n",
      "299 loss: 0.004927261732518673\n",
      "300 loss: 0.0046992069110274315\n",
      "301 loss: 0.004478109069168568\n",
      "302 loss: 0.00427226210013032\n",
      "303 loss: 0.004072044976055622\n",
      "304 loss: 0.003888251492753625\n",
      "305 loss: 0.003708156757056713\n",
      "306 loss: 0.0035407268442213535\n",
      "307 loss: 0.0033782306127250195\n",
      "308 loss: 0.0032243237365037203\n",
      "309 loss: 0.0030767216812819242\n",
      "310 loss: 0.002944865496829152\n",
      "311 loss: 0.002811925485730171\n",
      "312 loss: 0.0026864774990826845\n",
      "313 loss: 0.002566625364124775\n",
      "314 loss: 0.0024560026358813047\n",
      "315 loss: 0.002348504029214382\n",
      "316 loss: 0.0022477158345282078\n",
      "317 loss: 0.0021519267465919256\n",
      "318 loss: 0.002062723273411393\n",
      "319 loss: 0.001972768222913146\n",
      "320 loss: 0.0018884409219026566\n",
      "321 loss: 0.0018076010746881366\n",
      "322 loss: 0.0017342427745461464\n",
      "323 loss: 0.0016618515364825726\n",
      "324 loss: 0.0015933822141960263\n",
      "325 loss: 0.0015262505039572716\n",
      "326 loss: 0.0014669749652966857\n",
      "327 loss: 0.0014075406361371279\n",
      "328 loss: 0.001348836813122034\n",
      "329 loss: 0.001294586225412786\n",
      "330 loss: 0.0012419336708262563\n",
      "331 loss: 0.0011928713647648692\n",
      "332 loss: 0.0011464653071016073\n",
      "333 loss: 0.0011011454043909907\n",
      "334 loss: 0.001059719710610807\n",
      "335 loss: 0.0010199850657954812\n",
      "336 loss: 0.0009809151524677873\n",
      "337 loss: 0.0009442297159694135\n",
      "338 loss: 0.0009081574389711022\n",
      "339 loss: 0.0008749636472202837\n",
      "340 loss: 0.0008432507165707648\n",
      "341 loss: 0.0008121619466692209\n",
      "342 loss: 0.0007836467702873051\n",
      "343 loss: 0.0007529608556069434\n",
      "344 loss: 0.0007286716718226671\n",
      "345 loss: 0.0007028724648989737\n",
      "346 loss: 0.0006783190183341503\n",
      "347 loss: 0.0006537135341204703\n",
      "348 loss: 0.0006317660445347428\n",
      "349 loss: 0.0006079187733121216\n",
      "350 loss: 0.0005889564054086804\n",
      "351 loss: 0.0005690817488357425\n",
      "352 loss: 0.0005484736175276339\n",
      "353 loss: 0.0005311055574566126\n",
      "354 loss: 0.0005142744048498571\n",
      "355 loss: 0.0004972132155671716\n",
      "356 loss: 0.00048086559399962425\n",
      "357 loss: 0.0004640642728190869\n",
      "358 loss: 0.00044921523658558726\n",
      "359 loss: 0.00043451989768072963\n",
      "360 loss: 0.0004218452959321439\n",
      "361 loss: 0.0004097573983017355\n",
      "362 loss: 0.0003956229193136096\n",
      "363 loss: 0.0003844001330435276\n",
      "364 loss: 0.0003725726273842156\n",
      "365 loss: 0.0003614449524320662\n",
      "366 loss: 0.00035040598595514894\n",
      "367 loss: 0.00033935741521418095\n",
      "368 loss: 0.0003286758728791028\n",
      "369 loss: 0.0003201002546120435\n",
      "370 loss: 0.0003111091209575534\n",
      "371 loss: 0.0003021728480234742\n",
      "372 loss: 0.00029366964008659124\n",
      "373 loss: 0.0002857560175471008\n",
      "374 loss: 0.0002766445395536721\n",
      "375 loss: 0.0002690525143407285\n",
      "376 loss: 0.00026182274450547993\n",
      "377 loss: 0.00025434617418795824\n",
      "378 loss: 0.000247388263233006\n",
      "379 loss: 0.00024016427050810307\n",
      "380 loss: 0.0002342518127989024\n",
      "381 loss: 0.0002275063598062843\n",
      "382 loss: 0.0002219564630649984\n",
      "383 loss: 0.00021612347336485982\n",
      "384 loss: 0.00021101858874317259\n",
      "385 loss: 0.00020527796004898846\n",
      "386 loss: 0.00020024697005283087\n",
      "387 loss: 0.00019489266560412943\n",
      "388 loss: 0.00019096877076663077\n",
      "389 loss: 0.0001860054035205394\n",
      "390 loss: 0.00018162433116231114\n",
      "391 loss: 0.0001770019589457661\n",
      "392 loss: 0.00017308449605479836\n",
      "393 loss: 0.00016892080020625144\n",
      "394 loss: 0.0001653166691539809\n",
      "395 loss: 0.0001609300816198811\n",
      "396 loss: 0.00015646585961803794\n",
      "397 loss: 0.0001528994325781241\n",
      "398 loss: 0.00014950572222005576\n",
      "399 loss: 0.0001463030930608511\n",
      "400 loss: 0.00014307709352578968\n",
      "401 loss: 0.00013937900075688958\n",
      "402 loss: 0.00013607424625661224\n",
      "403 loss: 0.00013341446174308658\n",
      "404 loss: 0.00013074206071905792\n",
      "405 loss: 0.00012803665595129132\n",
      "406 loss: 0.0001247696636710316\n",
      "407 loss: 0.00012208959378767759\n",
      "408 loss: 0.00011926142906304449\n",
      "409 loss: 0.00011669614468701184\n",
      "410 loss: 0.00011436962813604623\n",
      "411 loss: 0.0001119687658501789\n",
      "412 loss: 0.00010987327550537884\n",
      "413 loss: 0.0001080627553164959\n",
      "414 loss: 0.00010562587704043835\n",
      "415 loss: 0.00010343033500248566\n",
      "416 loss: 0.00010093153832713142\n",
      "417 loss: 9.886952466331422e-05\n",
      "418 loss: 9.736567153595388e-05\n",
      "419 loss: 9.530116221867502e-05\n",
      "420 loss: 9.322552068624645e-05\n",
      "421 loss: 9.168052929453552e-05\n",
      "422 loss: 9.021098958328366e-05\n",
      "423 loss: 8.820258517516777e-05\n",
      "424 loss: 8.672753028804436e-05\n",
      "425 loss: 8.518729009665549e-05\n",
      "426 loss: 8.358273771591485e-05\n",
      "427 loss: 8.21037610876374e-05\n",
      "428 loss: 8.056381193455309e-05\n",
      "429 loss: 7.888205436756834e-05\n",
      "430 loss: 7.740717410342768e-05\n",
      "431 loss: 7.601307879667729e-05\n",
      "432 loss: 7.453636499121785e-05\n",
      "433 loss: 7.303405436687171e-05\n",
      "434 loss: 7.189031748566777e-05\n",
      "435 loss: 7.026971434243023e-05\n",
      "436 loss: 6.929656228749081e-05\n",
      "437 loss: 6.826777826063335e-05\n",
      "438 loss: 6.697756180074066e-05\n",
      "439 loss: 6.596387538593262e-05\n",
      "440 loss: 6.488738290499896e-05\n",
      "441 loss: 6.415457755792886e-05\n",
      "442 loss: 6.290970486588776e-05\n",
      "443 loss: 6.177086470415816e-05\n",
      "444 loss: 6.072608448448591e-05\n",
      "445 loss: 5.9904319641646e-05\n",
      "446 loss: 5.9009002143284306e-05\n",
      "447 loss: 5.775439421995543e-05\n",
      "448 loss: 5.695577056030743e-05\n",
      "449 loss: 5.592883826466277e-05\n",
      "450 loss: 5.512065035873093e-05\n",
      "451 loss: 5.412445534602739e-05\n",
      "452 loss: 5.325294841895811e-05\n",
      "453 loss: 5.253154449746944e-05\n",
      "454 loss: 5.161423177924007e-05\n",
      "455 loss: 5.092403807793744e-05\n",
      "456 loss: 4.989757144358009e-05\n",
      "457 loss: 4.945643377141096e-05\n",
      "458 loss: 4.8740621423348784e-05\n",
      "459 loss: 4.7896137402858585e-05\n",
      "460 loss: 4.717135743703693e-05\n",
      "461 loss: 4.666680615628138e-05\n",
      "462 loss: 4.566380812320858e-05\n",
      "463 loss: 4.509956488618627e-05\n",
      "464 loss: 4.441647615749389e-05\n",
      "465 loss: 4.367304063634947e-05\n",
      "466 loss: 4.326335692894645e-05\n",
      "467 loss: 4.2620136810000986e-05\n",
      "468 loss: 4.20999713242054e-05\n",
      "469 loss: 4.1585968574509025e-05\n",
      "470 loss: 4.091838491149247e-05\n",
      "471 loss: 4.03867379645817e-05\n",
      "472 loss: 3.9922921132529154e-05\n",
      "473 loss: 3.930610910174437e-05\n",
      "474 loss: 3.875063339364715e-05\n",
      "475 loss: 3.823166116490029e-05\n",
      "476 loss: 3.7885001802351326e-05\n",
      "477 loss: 3.7287332816049457e-05\n",
      "478 loss: 3.691900201374665e-05\n",
      "479 loss: 3.641444709501229e-05\n",
      "480 loss: 3.5741646570386365e-05\n",
      "481 loss: 3.551785630406812e-05\n",
      "482 loss: 3.4998949558939785e-05\n",
      "483 loss: 3.450654185144231e-05\n",
      "484 loss: 3.4058182791341096e-05\n",
      "485 loss: 3.3388438168913126e-05\n",
      "486 loss: 3.305187783553265e-05\n",
      "487 loss: 3.261124948039651e-05\n",
      "488 loss: 3.216500044800341e-05\n",
      "489 loss: 3.166010355926119e-05\n",
      "490 loss: 3.1348681659437716e-05\n",
      "491 loss: 3.097443186561577e-05\n",
      "492 loss: 3.0537925340468064e-05\n",
      "493 loss: 3.021425254701171e-05\n",
      "494 loss: 2.9911949241068214e-05\n",
      "495 loss: 2.9563158022938296e-05\n",
      "496 loss: 2.9127999368938617e-05\n",
      "497 loss: 2.885327376134228e-05\n",
      "498 loss: 2.8510035917861387e-05\n",
      "499 loss: 2.8160538931842893e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    我们可以通过建立torch.autograd的子类来实现我们自定义的autograd函数，\n",
    "    并完成张量的正向和反向传播。\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "        在正向传播中，我们接收到一个上下文对象和一个包含输入的张量；\n",
    "        我们必须返回一个包含输出的张量，\n",
    "        并且我们可以使用上下文对象来缓存对象，以便在反向传播中使用。\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(x)\n",
    "        return x.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        在反向传播中，我们接收到上下文对象和一个张量，\n",
    "        其包含了相对于正向传播过程中产生的输出的损失的梯度。\n",
    "        我们可以从上下文对象中检索缓存的数据，\n",
    "        并且必须计算并返回与正向传播的输入相关的损失的梯度。\n",
    "        \"\"\"\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_x = grad_output.clone()\n",
    "        grad_x[x < 0] = 0\n",
    "        return grad_x\n",
    " \n",
    " \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    "# N是批大小；D_in 是输入维度；\n",
    "# H 是隐藏层维度；D_out 是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    " \n",
    "# 产生输入和输出的随机张量\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    " \n",
    "# 产生随机权重的张量\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    " \n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 正向传播：使用张量上的操作来计算输出值y；\n",
    "    # 我们通过调用 MyReLU.apply 函数来使用自定义的ReLU\n",
    "    y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # 计算并输出loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(f'{t} loss: {loss.item()}')\n",
    "\n",
    "    # 使用autograd计算反向传播过程。\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 用梯度下降更新权重\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # 在反向传播之后手动清零梯度\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414e466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e4903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "344bf060",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:50:33.389102Z",
     "start_time": "2021-08-06T12:50:31.043026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "        module name  input shape output shape       params memory(MB)              MAdd            Flops   MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
      "0        features.0    3 224 224   64 224 224       1792.0      12.25     173,408,256.0     89,915,392.0     609280.0   12845056.0       7.73%   13454336.0\n",
      "1        features.1   64 224 224   64 224 224          0.0      12.25       3,211,264.0      3,211,264.0   12845056.0   12845056.0       0.86%   25690112.0\n",
      "2        features.2   64 224 224   64 112 112          0.0       3.06       2,408,448.0      3,211,264.0   12845056.0    3211264.0      11.99%   16056320.0\n",
      "3        features.3   64 112 112  128 112 112      73856.0       6.12   1,849,688,064.0    926,449,664.0    3506688.0    6422528.0       8.42%    9929216.0\n",
      "4        features.4  128 112 112  128 112 112          0.0       6.12       1,605,632.0      1,605,632.0    6422528.0    6422528.0       0.33%   12845056.0\n",
      "5        features.5  128 112 112  128  56  56          0.0       1.53       1,204,224.0      1,605,632.0    6422528.0    1605632.0       4.21%    8028160.0\n",
      "6        features.6  128  56  56  256  56  56     295168.0       3.06   1,849,688,064.0    925,646,848.0    2786304.0    3211264.0       7.38%    5997568.0\n",
      "7        features.7  256  56  56  256  56  56          0.0       3.06         802,816.0        802,816.0    3211264.0    3211264.0       0.09%    6422528.0\n",
      "8        features.8  256  56  56  256  56  56     590080.0       3.06   3,699,376,128.0  1,850,490,880.0    5571584.0    3211264.0      12.86%    8782848.0\n",
      "9        features.9  256  56  56  256  56  56          0.0       3.06         802,816.0        802,816.0    3211264.0    3211264.0       0.08%    6422528.0\n",
      "10      features.10  256  56  56  256  28  28          0.0       0.77         602,112.0        802,816.0    3211264.0     802816.0       2.13%    4014080.0\n",
      "11      features.11  256  28  28  512  28  28    1180160.0       1.53   1,849,688,064.0    925,245,440.0    5523456.0    1605632.0       4.29%    7129088.0\n",
      "12      features.12  512  28  28  512  28  28          0.0       1.53         401,408.0        401,408.0    1605632.0    1605632.0       0.05%    3211264.0\n",
      "13      features.13  512  28  28  512  28  28    2359808.0       1.53   3,699,376,128.0  1,850,089,472.0   11044864.0    1605632.0       7.96%   12650496.0\n",
      "14      features.14  512  28  28  512  28  28          0.0       1.53         401,408.0        401,408.0    1605632.0    1605632.0       0.05%    3211264.0\n",
      "15      features.15  512  28  28  512  14  14          0.0       0.38         301,056.0        401,408.0    1605632.0     401408.0       1.12%    2007040.0\n",
      "16      features.16  512  14  14  512  14  14    2359808.0       0.38     924,844,032.0    462,522,368.0    9840640.0     401408.0       3.00%   10242048.0\n",
      "17      features.17  512  14  14  512  14  14          0.0       0.38         100,352.0        100,352.0     401408.0     401408.0       0.03%     802816.0\n",
      "18      features.18  512  14  14  512  14  14    2359808.0       0.38     924,844,032.0    462,522,368.0    9840640.0     401408.0       2.82%   10242048.0\n",
      "19      features.19  512  14  14  512  14  14          0.0       0.38         100,352.0        100,352.0     401408.0     401408.0       0.03%     802816.0\n",
      "20      features.20  512  14  14  512   7   7          0.0       0.10          75,264.0        100,352.0     401408.0     100352.0       0.30%     501760.0\n",
      "21          avgpool  512   7   7  512   7   7          0.0       0.10               0.0              0.0          0.0          0.0       2.82%          0.0\n",
      "22     classifier.0        25088         4096  102764544.0       0.02     205,516,800.0    102,760,448.0  411158528.0      16384.0      16.93%  411174912.0\n",
      "23     classifier.1         4096         4096          0.0       0.02           4,096.0          4,096.0      16384.0      16384.0       0.03%      32768.0\n",
      "24     classifier.2         4096         4096          0.0       0.02               0.0              0.0          0.0          0.0       1.38%          0.0\n",
      "25     classifier.3         4096         4096   16781312.0       0.02      33,550,336.0     16,777,216.0   67141632.0      16384.0       2.46%   67158016.0\n",
      "26     classifier.4         4096         4096          0.0       0.02           4,096.0          4,096.0      16384.0      16384.0       0.01%      32768.0\n",
      "27     classifier.5         4096         4096          0.0       0.02               0.0              0.0          0.0          0.0       0.01%          0.0\n",
      "28     classifier.6         4096         1000    4097000.0       0.00       8,191,000.0      4,096,000.0   16404384.0       4000.0       0.63%   16408384.0\n",
      "total                                          132863336.0      62.69  15,230,196,248.0  7,630,071,808.0   16404384.0       4000.0     100.00%  663250240.0\n",
      "===========================================================================================================================================================\n",
      "Total params: 132,863,336\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 62.69MB\n",
      "Total MAdd: 15.23GMAdd\n",
      "Total Flops: 7.63GFlops\n",
      "Total MemR+W: 632.52MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch              \n",
    "from torchstat import stat           \n",
    "import torchvision.models as models      \n",
    "net = models.vgg11()           \n",
    "stat(net,(3,224,224))    # (3,224,224)表示输入图片的尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175dc63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f1e89e",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e5e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cf7dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d052aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca7bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2bb791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba2071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7c74d1dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T04:55:58.470051Z",
     "start_time": "2021-08-09T04:55:58.464118Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    " \n",
    "class MyReLUF(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    我们可以通过建立torch.autograd的子类来实现我们自定义的autograd函数，\n",
    "    并完成张量的正向和反向传播。\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "        在正向传播中，我们接收到一个上下文对象和一个包含输入的张量；\n",
    "        我们必须返回一个包含输出的张量，\n",
    "        并且我们可以使用上下文对象来缓存对象，以便在反向传播中使用。\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(x)\n",
    "        return x.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        在反向传播中，我们接收到上下文对象和一个张量，\n",
    "        其包含了相对于正向传播过程中产生的输出的损失的梯度。\n",
    "        我们可以从上下文对象中检索缓存的数据，\n",
    "        并且必须计算并返回与正向传播的输入相关的损失的梯度。\n",
    "        \"\"\"\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_x = grad_output.clone()\n",
    "        grad_x[x < 0] = 0\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "daef992a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T04:55:58.779169Z",
     "start_time": "2021-08-09T04:55:58.767922Z"
    }
   },
   "outputs": [],
   "source": [
    "class Relu(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(Relu, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "        self.w1 = nn.Parameter(torch.randn(D_in, H, device=device, requires_grad=True))\n",
    "        self.w2 = nn.Parameter(torch.randn(H, D_out, device=device, requires_grad=True))\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        self.w1.data.uniform_(-0.1, 0.1)\n",
    "        self.w2.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # See the autograd section for explanation of what happens here.\n",
    "        return MyReLUF.apply(input.mm(self.w1)).mm(self.w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b3e05eb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T04:57:15.304143Z",
     "start_time": "2021-08-09T04:57:14.863044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 972.7138061523438\n",
      "1 loss: 969.7625122070312\n",
      "2 loss: 966.84326171875\n",
      "3 loss: 963.9549560546875\n",
      "4 loss: 961.096923828125\n",
      "5 loss: 958.2689819335938\n",
      "6 loss: 955.470458984375\n",
      "7 loss: 952.7012939453125\n",
      "8 loss: 949.960693359375\n",
      "9 loss: 947.2482299804688\n",
      "10 loss: 944.5635986328125\n",
      "11 loss: 941.906494140625\n",
      "12 loss: 939.2763061523438\n",
      "13 loss: 936.6727294921875\n",
      "14 loss: 934.0953369140625\n",
      "15 loss: 931.543701171875\n",
      "16 loss: 929.017578125\n",
      "17 loss: 926.516357421875\n",
      "18 loss: 924.0399780273438\n",
      "19 loss: 921.587890625\n",
      "20 loss: 919.1597900390625\n",
      "21 loss: 916.7552490234375\n",
      "22 loss: 914.3740234375\n",
      "23 loss: 912.0156860351562\n",
      "24 loss: 909.6799926757812\n",
      "25 loss: 907.3665771484375\n",
      "26 loss: 905.0767822265625\n",
      "27 loss: 902.809814453125\n",
      "28 loss: 900.5640258789062\n",
      "29 loss: 898.3392944335938\n",
      "30 loss: 896.13525390625\n",
      "31 loss: 893.95166015625\n",
      "32 loss: 891.7882690429688\n",
      "33 loss: 889.644775390625\n",
      "34 loss: 887.520751953125\n",
      "35 loss: 885.4163208007812\n",
      "36 loss: 883.3316040039062\n",
      "37 loss: 881.265625\n",
      "38 loss: 879.2179565429688\n",
      "39 loss: 877.1885986328125\n",
      "40 loss: 875.1769409179688\n",
      "41 loss: 873.1829223632812\n",
      "42 loss: 871.2064208984375\n",
      "43 loss: 869.2469482421875\n",
      "44 loss: 867.304443359375\n",
      "45 loss: 865.3785400390625\n",
      "46 loss: 863.4691772460938\n",
      "47 loss: 861.575927734375\n",
      "48 loss: 859.6986694335938\n",
      "49 loss: 857.8370971679688\n",
      "50 loss: 855.9910888671875\n",
      "51 loss: 854.1603393554688\n",
      "52 loss: 852.3446655273438\n",
      "53 loss: 850.54443359375\n",
      "54 loss: 848.7587890625\n",
      "55 loss: 846.9876708984375\n",
      "56 loss: 845.2305908203125\n",
      "57 loss: 843.4877319335938\n",
      "58 loss: 841.7584838867188\n",
      "59 loss: 840.0430297851562\n",
      "60 loss: 838.3410034179688\n",
      "61 loss: 836.6522216796875\n",
      "62 loss: 834.9766845703125\n",
      "63 loss: 833.313720703125\n",
      "64 loss: 831.66357421875\n",
      "65 loss: 830.0260009765625\n",
      "66 loss: 828.4007568359375\n",
      "67 loss: 826.787841796875\n",
      "68 loss: 825.1868286132812\n",
      "69 loss: 823.5978393554688\n",
      "70 loss: 822.0206298828125\n",
      "71 loss: 820.4550170898438\n",
      "72 loss: 818.90087890625\n",
      "73 loss: 817.3577880859375\n",
      "74 loss: 815.8259887695312\n",
      "75 loss: 814.3049926757812\n",
      "76 loss: 812.7947998046875\n",
      "77 loss: 811.29541015625\n",
      "78 loss: 809.8062744140625\n",
      "79 loss: 808.32763671875\n",
      "80 loss: 806.8591918945312\n",
      "81 loss: 805.4010009765625\n",
      "82 loss: 803.9524536132812\n",
      "83 loss: 802.5139770507812\n",
      "84 loss: 801.0850830078125\n",
      "85 loss: 799.665771484375\n",
      "86 loss: 798.2560424804688\n",
      "87 loss: 796.8553466796875\n",
      "88 loss: 795.4640502929688\n",
      "89 loss: 794.0816650390625\n",
      "90 loss: 792.7083740234375\n",
      "91 loss: 791.3438720703125\n",
      "92 loss: 789.9888916015625\n",
      "93 loss: 788.6439819335938\n",
      "94 loss: 787.3075561523438\n",
      "95 loss: 785.9794311523438\n",
      "96 loss: 784.6596069335938\n",
      "97 loss: 783.3479614257812\n",
      "98 loss: 782.04541015625\n",
      "99 loss: 780.7512817382812\n",
      "100 loss: 779.465087890625\n",
      "101 loss: 778.1865234375\n",
      "102 loss: 776.9156494140625\n",
      "103 loss: 775.6524047851562\n",
      "104 loss: 774.3964233398438\n",
      "105 loss: 773.14794921875\n",
      "106 loss: 771.9067993164062\n",
      "107 loss: 770.6729736328125\n",
      "108 loss: 769.4464111328125\n",
      "109 loss: 768.226806640625\n",
      "110 loss: 767.01416015625\n",
      "111 loss: 765.8082885742188\n",
      "112 loss: 764.609130859375\n",
      "113 loss: 763.4166870117188\n",
      "114 loss: 762.230712890625\n",
      "115 loss: 761.05126953125\n",
      "116 loss: 759.8787231445312\n",
      "117 loss: 758.712158203125\n",
      "118 loss: 757.5523681640625\n",
      "119 loss: 756.39892578125\n",
      "120 loss: 755.2515258789062\n",
      "121 loss: 754.1101684570312\n",
      "122 loss: 752.9747314453125\n",
      "123 loss: 751.8450927734375\n",
      "124 loss: 750.7213745117188\n",
      "125 loss: 749.6032104492188\n",
      "126 loss: 748.4908447265625\n",
      "127 loss: 747.3843994140625\n",
      "128 loss: 746.2835693359375\n",
      "129 loss: 745.1883544921875\n",
      "130 loss: 744.098388671875\n",
      "131 loss: 743.0138549804688\n",
      "132 loss: 741.9344482421875\n",
      "133 loss: 740.8602294921875\n",
      "134 loss: 739.7911987304688\n",
      "135 loss: 738.727294921875\n",
      "136 loss: 737.6689453125\n",
      "137 loss: 736.6156005859375\n",
      "138 loss: 735.5676879882812\n",
      "139 loss: 734.5244750976562\n",
      "140 loss: 733.4861450195312\n",
      "141 loss: 732.4525146484375\n",
      "142 loss: 731.423583984375\n",
      "143 loss: 730.3992919921875\n",
      "144 loss: 729.37939453125\n",
      "145 loss: 728.364013671875\n",
      "146 loss: 727.3530883789062\n",
      "147 loss: 726.3466186523438\n",
      "148 loss: 725.3444213867188\n",
      "149 loss: 724.3465576171875\n",
      "150 loss: 723.35302734375\n",
      "151 loss: 722.3638305664062\n",
      "152 loss: 721.3787231445312\n",
      "153 loss: 720.397705078125\n",
      "154 loss: 719.4208984375\n",
      "155 loss: 718.4480590820312\n",
      "156 loss: 717.4793701171875\n",
      "157 loss: 716.5145263671875\n",
      "158 loss: 715.5535278320312\n",
      "159 loss: 714.5966186523438\n",
      "160 loss: 713.6436157226562\n",
      "161 loss: 712.6944580078125\n",
      "162 loss: 711.7489013671875\n",
      "163 loss: 710.8069458007812\n",
      "164 loss: 709.868896484375\n",
      "165 loss: 708.9344482421875\n",
      "166 loss: 708.00341796875\n",
      "167 loss: 707.0760498046875\n",
      "168 loss: 706.1522827148438\n",
      "169 loss: 705.231689453125\n",
      "170 loss: 704.3145141601562\n",
      "171 loss: 703.4007568359375\n",
      "172 loss: 702.4903564453125\n",
      "173 loss: 701.5831909179688\n",
      "174 loss: 700.6793823242188\n",
      "175 loss: 699.7787475585938\n",
      "176 loss: 698.8814086914062\n",
      "177 loss: 697.9871826171875\n",
      "178 loss: 697.0960083007812\n",
      "179 loss: 696.2079467773438\n",
      "180 loss: 695.3231201171875\n",
      "181 loss: 694.441162109375\n",
      "182 loss: 693.562255859375\n",
      "183 loss: 692.686279296875\n",
      "184 loss: 691.8132934570312\n",
      "185 loss: 690.943359375\n",
      "186 loss: 690.0763549804688\n",
      "187 loss: 689.2122802734375\n",
      "188 loss: 688.3510131835938\n",
      "189 loss: 687.4925537109375\n",
      "190 loss: 686.636962890625\n",
      "191 loss: 685.7839965820312\n",
      "192 loss: 684.933837890625\n",
      "193 loss: 684.0864868164062\n",
      "194 loss: 683.2418212890625\n",
      "195 loss: 682.3995971679688\n",
      "196 loss: 681.5601196289062\n",
      "197 loss: 680.72314453125\n",
      "198 loss: 679.8888549804688\n",
      "199 loss: 679.0570678710938\n",
      "200 loss: 678.2278442382812\n",
      "201 loss: 677.4010620117188\n",
      "202 loss: 676.5767822265625\n",
      "203 loss: 675.7553100585938\n",
      "204 loss: 674.9361572265625\n",
      "205 loss: 674.1194458007812\n",
      "206 loss: 673.3050537109375\n",
      "207 loss: 672.4931030273438\n",
      "208 loss: 671.68359375\n",
      "209 loss: 670.8763427734375\n",
      "210 loss: 670.0717163085938\n",
      "211 loss: 669.2692260742188\n",
      "212 loss: 668.4690551757812\n",
      "213 loss: 667.6710815429688\n",
      "214 loss: 666.8753662109375\n",
      "215 loss: 666.081787109375\n",
      "216 loss: 665.2904052734375\n",
      "217 loss: 664.501220703125\n",
      "218 loss: 663.7142333984375\n",
      "219 loss: 662.9293212890625\n",
      "220 loss: 662.146484375\n",
      "221 loss: 661.3657836914062\n",
      "222 loss: 660.587158203125\n",
      "223 loss: 659.810546875\n",
      "224 loss: 659.0361328125\n",
      "225 loss: 658.2636108398438\n",
      "226 loss: 657.4932861328125\n",
      "227 loss: 656.7249755859375\n",
      "228 loss: 655.958740234375\n",
      "229 loss: 655.1943359375\n",
      "230 loss: 654.431884765625\n",
      "231 loss: 653.6715087890625\n",
      "232 loss: 652.9133911132812\n",
      "233 loss: 652.1575927734375\n",
      "234 loss: 651.4036865234375\n",
      "235 loss: 650.6516723632812\n",
      "236 loss: 649.9014892578125\n",
      "237 loss: 649.1531982421875\n",
      "238 loss: 648.4065551757812\n",
      "239 loss: 647.661865234375\n",
      "240 loss: 646.9188842773438\n",
      "241 loss: 646.177734375\n",
      "242 loss: 645.4383544921875\n",
      "243 loss: 644.7007446289062\n",
      "244 loss: 643.9649047851562\n",
      "245 loss: 643.2307739257812\n",
      "246 loss: 642.498291015625\n",
      "247 loss: 641.7677001953125\n",
      "248 loss: 641.0386352539062\n",
      "249 loss: 640.311279296875\n",
      "250 loss: 639.5855102539062\n",
      "251 loss: 638.8616333007812\n",
      "252 loss: 638.1392822265625\n",
      "253 loss: 637.41845703125\n",
      "254 loss: 636.6993408203125\n",
      "255 loss: 635.9817504882812\n",
      "256 loss: 635.2662963867188\n",
      "257 loss: 634.5523681640625\n",
      "258 loss: 633.8400268554688\n",
      "259 loss: 633.1292114257812\n",
      "260 loss: 632.419921875\n",
      "261 loss: 631.7130126953125\n",
      "262 loss: 631.0076293945312\n",
      "263 loss: 630.3036499023438\n",
      "264 loss: 629.6013793945312\n",
      "265 loss: 628.9009399414062\n",
      "266 loss: 628.2020263671875\n",
      "267 loss: 627.5045166015625\n",
      "268 loss: 626.8086547851562\n",
      "269 loss: 626.114013671875\n",
      "270 loss: 625.4208984375\n",
      "271 loss: 624.729248046875\n",
      "272 loss: 624.0391845703125\n",
      "273 loss: 623.3504638671875\n",
      "274 loss: 622.6632080078125\n",
      "275 loss: 621.9774169921875\n",
      "276 loss: 621.29296875\n",
      "277 loss: 620.6099243164062\n",
      "278 loss: 619.92822265625\n",
      "279 loss: 619.2479248046875\n",
      "280 loss: 618.569091796875\n",
      "281 loss: 617.8914794921875\n",
      "282 loss: 617.2152709960938\n",
      "283 loss: 616.540283203125\n",
      "284 loss: 615.8667602539062\n",
      "285 loss: 615.1944580078125\n",
      "286 loss: 614.5235595703125\n",
      "287 loss: 613.8538818359375\n",
      "288 loss: 613.1854858398438\n",
      "289 loss: 612.5183715820312\n",
      "290 loss: 611.8529052734375\n",
      "291 loss: 611.1892700195312\n",
      "292 loss: 610.527099609375\n",
      "293 loss: 609.8662719726562\n",
      "294 loss: 609.20703125\n",
      "295 loss: 608.549072265625\n",
      "296 loss: 607.8922119140625\n",
      "297 loss: 607.236572265625\n",
      "298 loss: 606.5821533203125\n",
      "299 loss: 605.9288940429688\n",
      "300 loss: 605.2769775390625\n",
      "301 loss: 604.6261596679688\n",
      "302 loss: 603.9765625\n",
      "303 loss: 603.3281860351562\n",
      "304 loss: 602.6809692382812\n",
      "305 loss: 602.0348510742188\n",
      "306 loss: 601.3899536132812\n",
      "307 loss: 600.7461547851562\n",
      "308 loss: 600.103515625\n",
      "309 loss: 599.462158203125\n",
      "310 loss: 598.8217163085938\n",
      "311 loss: 598.1826171875\n",
      "312 loss: 597.54443359375\n",
      "313 loss: 596.907470703125\n",
      "314 loss: 596.271728515625\n",
      "315 loss: 595.63720703125\n",
      "316 loss: 595.0035400390625\n",
      "317 loss: 594.3712158203125\n",
      "318 loss: 593.7398681640625\n",
      "319 loss: 593.1096801757812\n",
      "320 loss: 592.48095703125\n",
      "321 loss: 591.8533935546875\n",
      "322 loss: 591.2269897460938\n",
      "323 loss: 590.6016235351562\n",
      "324 loss: 589.9771728515625\n",
      "325 loss: 589.3538818359375\n",
      "326 loss: 588.7316284179688\n",
      "327 loss: 588.1105346679688\n",
      "328 loss: 587.4903564453125\n",
      "329 loss: 586.8712768554688\n",
      "330 loss: 586.253173828125\n",
      "331 loss: 585.6362915039062\n",
      "332 loss: 585.0203857421875\n",
      "333 loss: 584.405517578125\n",
      "334 loss: 583.7916259765625\n",
      "335 loss: 583.1788330078125\n",
      "336 loss: 582.5670776367188\n",
      "337 loss: 581.9562377929688\n",
      "338 loss: 581.3464965820312\n",
      "339 loss: 580.7376708984375\n",
      "340 loss: 580.1297607421875\n",
      "341 loss: 579.5228881835938\n",
      "342 loss: 578.9171142578125\n",
      "343 loss: 578.312255859375\n",
      "344 loss: 577.7083129882812\n",
      "345 loss: 577.1053466796875\n",
      "346 loss: 576.5033569335938\n",
      "347 loss: 575.9024047851562\n",
      "348 loss: 575.3023071289062\n",
      "349 loss: 574.7032470703125\n",
      "350 loss: 574.105224609375\n",
      "351 loss: 573.5079956054688\n",
      "352 loss: 572.9116821289062\n",
      "353 loss: 572.3162231445312\n",
      "354 loss: 571.7218017578125\n",
      "355 loss: 571.12841796875\n",
      "356 loss: 570.5359497070312\n",
      "357 loss: 569.9443359375\n",
      "358 loss: 569.3535766601562\n",
      "359 loss: 568.7637329101562\n",
      "360 loss: 568.1749267578125\n",
      "361 loss: 567.5870361328125\n",
      "362 loss: 567.0000610351562\n",
      "363 loss: 566.4140625\n",
      "364 loss: 565.8291015625\n",
      "365 loss: 565.2449951171875\n",
      "366 loss: 564.6617431640625\n",
      "367 loss: 564.0794677734375\n",
      "368 loss: 563.498046875\n",
      "369 loss: 562.91748046875\n",
      "370 loss: 562.3377075195312\n",
      "371 loss: 561.7589111328125\n",
      "372 loss: 561.1810302734375\n",
      "373 loss: 560.60400390625\n",
      "374 loss: 560.02783203125\n",
      "375 loss: 559.4524536132812\n",
      "376 loss: 558.8779907226562\n",
      "377 loss: 558.3043212890625\n",
      "378 loss: 557.7315673828125\n",
      "379 loss: 557.15966796875\n",
      "380 loss: 556.588623046875\n",
      "381 loss: 556.0183715820312\n",
      "382 loss: 555.448974609375\n",
      "383 loss: 554.8804931640625\n",
      "384 loss: 554.3128051757812\n",
      "385 loss: 553.7459106445312\n",
      "386 loss: 553.1798706054688\n",
      "387 loss: 552.6146240234375\n",
      "388 loss: 552.0502319335938\n",
      "389 loss: 551.4867553710938\n",
      "390 loss: 550.9240112304688\n",
      "391 loss: 550.3621215820312\n",
      "392 loss: 549.801025390625\n",
      "393 loss: 549.24072265625\n",
      "394 loss: 548.6812133789062\n",
      "395 loss: 548.12255859375\n",
      "396 loss: 547.564697265625\n",
      "397 loss: 547.0076293945312\n",
      "398 loss: 546.4512939453125\n",
      "399 loss: 545.8958740234375\n",
      "400 loss: 545.3411865234375\n",
      "401 loss: 544.787353515625\n",
      "402 loss: 544.234130859375\n",
      "403 loss: 543.6818237304688\n",
      "404 loss: 543.13037109375\n",
      "405 loss: 542.57958984375\n",
      "406 loss: 542.0296630859375\n",
      "407 loss: 541.4805297851562\n",
      "408 loss: 540.9321899414062\n",
      "409 loss: 540.384521484375\n",
      "410 loss: 539.8377685546875\n",
      "411 loss: 539.2921142578125\n",
      "412 loss: 538.7470703125\n",
      "413 loss: 538.2029418945312\n",
      "414 loss: 537.6597290039062\n",
      "415 loss: 537.1174926757812\n",
      "416 loss: 536.5758056640625\n",
      "417 loss: 536.0348510742188\n",
      "418 loss: 535.4949340820312\n",
      "419 loss: 534.9554443359375\n",
      "420 loss: 534.416748046875\n",
      "421 loss: 533.8788452148438\n",
      "422 loss: 533.341796875\n",
      "423 loss: 532.8053588867188\n",
      "424 loss: 532.2695922851562\n",
      "425 loss: 531.7348022460938\n",
      "426 loss: 531.2005615234375\n",
      "427 loss: 530.6670532226562\n",
      "428 loss: 530.134521484375\n",
      "429 loss: 529.6029052734375\n",
      "430 loss: 529.072021484375\n",
      "431 loss: 528.5418090820312\n",
      "432 loss: 528.0125122070312\n",
      "433 loss: 527.4837036132812\n",
      "434 loss: 526.9556884765625\n",
      "435 loss: 526.4284057617188\n",
      "436 loss: 525.90185546875\n",
      "437 loss: 525.3759765625\n",
      "438 loss: 524.8509521484375\n",
      "439 loss: 524.32666015625\n",
      "440 loss: 523.802978515625\n",
      "441 loss: 523.280029296875\n",
      "442 loss: 522.7578735351562\n",
      "443 loss: 522.236328125\n",
      "444 loss: 521.71533203125\n",
      "445 loss: 521.1952514648438\n",
      "446 loss: 520.6759033203125\n",
      "447 loss: 520.1570434570312\n",
      "448 loss: 519.638916015625\n",
      "449 loss: 519.1216430664062\n",
      "450 loss: 518.6049194335938\n",
      "451 loss: 518.0888671875\n",
      "452 loss: 517.5735473632812\n",
      "453 loss: 517.0589599609375\n",
      "454 loss: 516.5449829101562\n",
      "455 loss: 516.03173828125\n",
      "456 loss: 515.519287109375\n",
      "457 loss: 515.0074462890625\n",
      "458 loss: 514.4962158203125\n",
      "459 loss: 513.9857788085938\n",
      "460 loss: 513.4759521484375\n",
      "461 loss: 512.9666748046875\n",
      "462 loss: 512.4583129882812\n",
      "463 loss: 511.9508056640625\n",
      "464 loss: 511.4439697265625\n",
      "465 loss: 510.9378356933594\n",
      "466 loss: 510.4324035644531\n",
      "467 loss: 509.92767333984375\n",
      "468 loss: 509.4234619140625\n",
      "469 loss: 508.9199523925781\n",
      "470 loss: 508.4172058105469\n",
      "471 loss: 507.9150085449219\n",
      "472 loss: 507.4134216308594\n",
      "473 loss: 506.91259765625\n",
      "474 loss: 506.4124450683594\n",
      "475 loss: 505.912841796875\n",
      "476 loss: 505.4139099121094\n",
      "477 loss: 504.9156799316406\n",
      "478 loss: 504.4180603027344\n",
      "479 loss: 503.9210205078125\n",
      "480 loss: 503.4247131347656\n",
      "481 loss: 502.9290771484375\n",
      "482 loss: 502.4339294433594\n",
      "483 loss: 501.9394226074219\n",
      "484 loss: 501.4456787109375\n",
      "485 loss: 500.952392578125\n",
      "486 loss: 500.4598693847656\n",
      "487 loss: 499.9681701660156\n",
      "488 loss: 499.47711181640625\n",
      "489 loss: 498.9868469238281\n",
      "490 loss: 498.4974060058594\n",
      "491 loss: 498.00872802734375\n",
      "492 loss: 497.5204772949219\n",
      "493 loss: 497.0329284667969\n",
      "494 loss: 496.54595947265625\n",
      "495 loss: 496.05975341796875\n",
      "496 loss: 495.5740051269531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497 loss: 495.0888366699219\n",
      "498 loss: 494.60443115234375\n",
      "499 loss: 494.12066650390625\n"
     ]
    }
   ],
   "source": [
    "def step(model, lr):\n",
    "    with torch.no_grad():\n",
    "        # 用梯度下降更新权重\n",
    "        model.w1 -= learning_rate * model.w1.grad\n",
    "        model.w2 -= learning_rate * model.w2.grad\n",
    "\n",
    "        # 在反向传播之后手动清零梯度\n",
    "        model.w1.grad.zero_()\n",
    "        model.w2.grad.zero_()\n",
    " \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    "# N是批大小；D_in 是输入维度；\n",
    "# H 是隐藏层维度；D_out 是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    " \n",
    "# 产生输入和输出的随机张量\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "relu = Relu(D_in, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 正向传播：使用张量上的操作来计算输出值y；\n",
    "    # 我们通过调用 MyReLU.apply 函数来使用自定义的ReLU\n",
    "    y_pred = relu(x)\n",
    "\n",
    "    # 计算并输出loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(f'{t} loss: {loss.item()}')\n",
    "\n",
    "    # 使用autograd计算反向传播过程。\n",
    "    loss.backward()\n",
    "\n",
    "    step(relu, learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e13179f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('object_tracking': conda)",
   "language": "python",
   "name": "python371064bitobjecttrackingconda66c6f2efc4d740808a4620e21504b8f3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
